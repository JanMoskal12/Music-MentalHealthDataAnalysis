mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
param_grids <- extract_parameter_set_dials(class_models) %>%
update(
trees = trees(range = c(100, 1000)),
min_n = min_n(range = c(2, 20)),
mtry = mtry(range = c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(range = c(-5, -1), trans = log10_trans()),
cost = cost(range = c(-3, 3), trans = log2_trans()),
rbf_sigma = rbf_sigma(range = c(-3, 0), trans = log2_trans())
)
param_grids <- extract_parameter_set_dials(class_models) %>%
update(
trees = trees(range = c(100, 1000)),
min_n = min_n(range = c(2, 20)),
mtry = mtry(range = c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(range = c(-5, -1), trans = log10_trans()),
cost = cost(range = c(-3, 3), trans = log2_trans()),
rbf_sigma = rbf_sigma(range = c(-3, 0), trans = log2_trans())
)
class_models <- readRDS("mxmh_models.rds")
param_grids <- extract_parameter_set_dials(class_models) %>%
update(
trees = trees(range = c(100, 1000)),
min_n = min_n(range = c(2, 20)),
mtry = mtry(range = c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(range = c(-5, -1), trans = log10_trans()),
cost = cost(range = c(-3, 3), trans = log2_trans()),
rbf_sigma = rbf_sigma(range = c(-3, 0), trans = log2_trans())
)
extract_parameter_set_dials(class_models)
class_models
class_models <- readRDS("mxmh_models.rds")
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
param_grids <- extract_parameter_set_dials(class_models) %>%
update(
trees = trees(range = c(100, 1000)),
min_n = min_n(range = c(2, 20)),
mtry = mtry(range = c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(range = c(-5, -1), trans = log10_trans()),
cost = cost(range = c(-3, 3), trans = log2_trans()),
rbf_sigma = rbf_sigma(range = c(-3, 0), trans = log2_trans())
)
library(dials)
# Przygotowanie listy workflowów
class_models <- readRDS("mxmh_models.rds")
wf_tbl <- class_models %>%
mutate(workflow = map(result, ~ extract_workflow(.x))) %>%
select(wflow_id, info, workflow)
controlBayes <- control_bayes(save_pred = TRUE, save_workflow = TRUE,
seed = 123, no_improve = 15, verbose = TRUE,
save_gp_scoring = TRUE, uncertain = 10)
wf_tbl <- class_models %>%
mutate(workflow = map(result, ~ extract_workflow(.x))) %>%
select(wflow_id, info, workflow)
results <- workflow_map(
wf_tbl,
fn = "tune_bayes",
resamples = cv_folds,
param_info = param_grids,
control = controlBayes,
iter = 50
)
library(tidymodels)
library(doParallel)
library(discrim)
library(mda)
library(dials)
library(tidymodels)
library(doParallel)
library(discrim)
library(mda)
library(dials)
df <- readRDS("data_no_missing_values.rds")
set.seed(123)
df_split <- initial_split(df, prop = 0.7, strata = "Depression")
df_train <- training(df_split)
df_test <- testing(df_split)
models <- list(
lda = discrim_linear(
regularization_method = "diagonal",
penalty = tune(),
mode = "classification") %>%
set_engine("mda"),
rf = rand_forest(
trees = tune(),
min_n = tune(),
mtry = tune(),
mode = "classification") %>%
set_engine("ranger"),
xgb = boost_tree(
trees = tune(),
min_n = tune(),
mtry = tune(),
learn_rate = tune(),
mode = "classification") %>%
set_engine("xgboost"),
svm = svm_rbf(
cost = tune(),
rbf_sigma = tune(),
mode = "classification") %>%
set_engine("kernlab")
)
recipes <- list(
rec_lda = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.8),
rec_rf = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors())  %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.8),
rec_xgb = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.6),
rec_svm = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.6)
)
class_models <- workflow_set(
preproc = recipes,
models = models,
cross = FALSE
)
cv_folds <- vfold_cv(df_train, v = 5, strata = "Depression")
param_grid <- list(
lda = parameters(
penalty()
),
rf = parameters(
trees(),
min_n(),
finalize(mtry(), df_train)
),
xgb = parameters(
trees(),
min_n(),
finalize(mtry(), df_train),
learn_rate()
),
svm = parameters(
cost(),
rbf_sigma()
)
)
# Przygotowanie listy workflowów
class_models <- readRDS("mxmh_models.rds")
registerDoParallel(cores = parallel::detectCores() - 1)
controlBayes <- control_bayes(save_pred = TRUE, save_workflow = TRUE,
seed = 123, no_improve = 15, verbose = TRUE,
save_gp_scoring = TRUE, uncertain = 10)
wf_tbl <- class_models %>%
mutate(workflow = map(result, ~ extract_workflow(.x))) %>%
select(wflow_id, info, workflow)
results <- workflow_map(
wf_tbl,
fn = "tune_bayes",
resamples = cv_folds,
param_info = param_grids,
control = controlBayes,
iter = 50
)
best_params <- map(class_models$result, ~select_best(.x, "accuracy"))
class_models$result
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
initial
wflows <- class_models %>%
pull(workflow)
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
wflows <- class_models %>%
pull(workflow)
class_models
library(purrr)
library(tune)
# `initial` i `param_grids` to listy o tej samej długości co `class_models`
wflows <- class_models$workflow
wflow_ids <- class_models$wflow_id
controlBayes <- control_bayes(save_pred = TRUE, save_workflow = TRUE)
class_models2 <- map2(
wflows,
seq_along(wflows),
~ tune_bayes(
object = .x,
resamples = cv_folds,
initial = tibble::as_tibble(initial[[.y]]),
iter = 50,
control = controlBayes,
param_info = param_grids[[.y]]
)
)
names(class_models2) <- wflow_ids
names(class_models2)
wflow_ids
class_models2
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
class_models2 <- workflow_map(
class_models,
fn = "tune_bayes",
initial = initial,
resamples = cv_folds,
iter = 50,
control = control_bayes(save_pred = TRUE, save_workflow = TRUE)
)
controlBayes <- control_bayes(save_pred = TRUE, save_workflow = TRUE,
seed = 123, no_improve = 15, verbose = TRUE,
save_gp_scoring = TRUE, uncertain = 10)
library(purrr)
library(tune)
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
registerDoParallel(cores = parallel::detectCores() - 1)
class_models2 <- workflow_map(
class_models,
fn = "tune_bayes",
initial = initial,
resamples = cv_folds,
iter = 50,
control = controlBayes,
verbose = TRUE
)
initial
extract_parameter_set_dials(class_models)
set.seed(123)
df_split <- initial_split(df, prop = 0.7, strata = "Depression")
df_train <- training(df_split)
df_test <- testing(df_split)
models <- list(
lda = discrim_linear(
regularization_method = "diagonal",
penalty = tune(),
mode = "classification") %>%
set_engine("mda"),
rf = rand_forest(
trees = tune(),
min_n = tune(),
mtry = tune(),
mode = "classification") %>%
set_engine("ranger"),
xgb = boost_tree(
trees = tune(),
min_n = tune(),
mtry = tune(),
learn_rate = tune(),
mode = "classification") %>%
set_engine("xgboost"),
svm = svm_rbf(
cost = tune(),
rbf_sigma = tune(),
mode = "classification") %>%
set_engine("kernlab")
)
recipes <- list(
rec_lda = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.8),
rec_rf = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors())  %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.8),
rec_xgb = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.6),
rec_svm = recipe(Depression ~ ., data = df_train) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_pca(all_predictors(), threshold = 0.6)
)
class_models <- workflow_set(
preproc = recipes,
models = models,
cross = FALSE
)
class_models2 <- class_models
cv_folds <- vfold_cv(df_train, v = 5, strata = "Depression")
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
# Przygotowanie listy workflowów
class_models <- readRDS("mxmh_models.rds")
controlBayes <- control_bayes(save_pred = TRUE, save_workflow = TRUE,
seed = 123, no_improve = 15, verbose = TRUE,
save_gp_scoring = TRUE, uncertain = 10)
initial <- class_models %>%
mutate(initial = map(result, ~ select_best(.x, metric = "accuracy"))) %>%
pull(initial)
registerDoParallel(cores = parallel::detectCores() - 1)
class_models2 <- workflow_map(
class_models2,
fn = "tune_bayes",
initial = initial,
resamples = cv_folds,
iter = 50,
control = controlBayes,
verbose = TRUE
)
initial
initial[2][1]
initial
initial[[3]]
initial[[3]][-4]
initial[[3]][-5]
initial[[2]]
initial[[2]][-4]
initial[[1]][-4]
initial[[1]]
initial[[4]]
class_models2 <- workflow_map(
class_models2,
"tune_bayes",
initial = list(
rec_rf_rf = initial[[2]][-4],
rec_xgb_xgb = initial[[3]][-5],
rec_svm_svm = initial[[4]][-3],
rec_lda_lda = initial[[1]][-2]
),
resamples = cv_folds,
param_info = param_grids,
iter = 50,
control = controlBayes
)
param_grid <- list(
lda = parameters(
penalty()
),
rf = parameters(
trees(),
min_n(),
finalize(mtry(), df_train)
),
xgb = parameters(
trees(),
min_n(),
finalize(mtry(), df_train),
learn_rate()
),
svm = parameters(
cost(),
rbf_sigma()
)
)
class_models2 <- workflow_map(
class_models2,
"tune_bayes",
initial = list(
rec_rf_rf = initial[[2]][-4],
rec_xgb_xgb = initial[[3]][-5],
rec_svm_svm = initial[[4]][-3],
rec_lda_lda = initial[[1]][-2]
),
resamples = cv_folds,
param_info = param_grid,
iter = 50,
control = controlBayes
)
class_models2 <- workflow_map(
class_models2,
"tune_bayes",
initial = list(
rec_rf_rf = initial[[2]][-4],
rec_xgb_xgb = initial[[3]][-5],
rec_svm_svm = initial[[4]][-3],
rec_lda_lda = initial[[1]][-2]
),
resamples = cv_folds,
param_info = param_grid,
iter = 50,
control = controlBayes,
verbose = TRUE
)
class_models2 <- workflow_map(
class_models2,
"tune_bayes",
initial = 5,
resamples = cv_folds,
param_info = param_grid,
iter = 50,
control = controlBayes,
verbose = TRUE
)
class_models2 <- workflow_map(
class_models2,
"tune_bayes",
initial = 10,
resamples = cv_folds,
param_info = param_grid,
iter = 50,
control = controlBayes,
verbose = TRUE
)
param_grids <- extract_parameter_set_dials(class_models) %>%
update(
trees = trees(c(100, 1000)),
min_n = min_n(c(2, 20)),
mtry = mtry(c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(c(-5, -1)),    # log10 scale
cost = cost(c(-3, 3)),                 # log2 scale
rbf_sigma = rbf_sigma(c(-3, 0))        # log2 scale
)
param_grids <- extract_parameter_set_dials(class_models2) %>%
update(
trees = trees(c(100, 1000)),
min_n = min_n(c(2, 20)),
mtry = mtry(c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(c(-5, -1)),    # log10 scale
cost = cost(c(-3, 3)),                 # log2 scale
rbf_sigma = rbf_sigma(c(-3, 0))        # log2 scale
)
class_models <- workflow_set(
preproc = recipes,
models = models,
cross = FALSE
)
class_models2 <- class_models
# Przygotowanie listy workflowów
class_models <- readRDS("mxmh_models.rds")
cv_folds <- vfold_cv(df_train, v = 5, strata = "Depression")
# Przygotowanie listy workflowów
class_models <- readRDS("mxmh_models.rds")
extract_parameter_set_dials(class_models2)
extract_parameter_set_dials(class_models1)
extract_parameter_set_dials(class_models)
param_grids <- list(
rec_rf_rf = extract_parameter_set_dials(class_models %>% extract_workflow_set_result("rec_rf_rf")) %>%
update(
trees = trees(c(100, 1000)),
min_n = min_n(c(2, 20)),
mtry = mtry(c(1, ncol(df_train) - 1))
),
rec_xgb_xgb = extract_parameter_set_dials(class_models %>% extract_workflow_set_result("rec_xgb_xgb")) %>%
update(
trees = trees(c(100, 1000)),
min_n = min_n(c(2, 20)),
mtry = mtry(c(1, ncol(df_train) - 1)),
learn_rate = learn_rate(c(-5, -1))
),
rec_svm_svm = extract_parameter_set_dials(class_models %>% extract_workflow_set_result("rec_svm_svm")) %>%
update(
cost = cost(c(-3, 3)),
rbf_sigma = rbf_sigma(c(-3, 0))
),
rec_lda_lda = extract_parameter_set_dials(class_models %>% extract_workflow_set_result("rec_lda_lda")) %>%
update(
penalty = penalty(c(0, 1))
)
)
extract_parameter_set_dials(class_models %>% extract_workflow_set_result("rec_rf_rf"))
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
setwd("C:/Users/szymo/OneDrive/Pulpit/Szymon/studia/Atlas/Music-MentalHealthDataAnalysis")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(scales)
colors <- c("#FFB300", "#803E75", "#FF6800", "#A6BDD7", "#C10020", "#CEA262", "#817066",
"#007D34", "#F6768E", "#00538A", "#FF7A5C", "#53377A", "#FF8E00", "#B32851",
"#F4C800", "#7F180D")
data <- readRDS("df_EDA.rds")
df <- readRDS("df.rds")
plot_distribution <- function(df, var, title, xlab, fill_colors = NULL){
df %>%
drop_na({{var}}) %>%
mutate({{var}} := fct_infreq({{var}})) %>%
ggplot(aes(x = {{var}}, fill = {{var}})) +
geom_bar() +
theme_minimal() +
theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = title, x = xlab, y = "Liczba osób")
}
plot_boxplot <- function(df, xvar, yvar, title, xlab, ylab){
df %>%
drop_na({{yvar}}) %>%
mutate({{xvar}} := factor({{xvar}}, levels = c("Low", "Medium", "High"))) %>%
ggplot(aes(x = {{xvar}}, y = {{yvar}}, fill = {{xvar}})) +
geom_boxplot() +
theme_minimal() +
theme(legend.position = "none") +
labs(title = title, x = xlab, y = ylab)
}
plot_prop_bar <- function(df, fill_var, title, xlab, ylab, fill_lab){
df %>%
drop_na(Depression, {{fill_var}}) %>%
mutate(Depression = factor(Depression, levels = c("Low", "Medium", "High"))) %>%
group_by(Depression, {{fill_var}}) %>%
summarise(count = n(), .groups = "drop") %>%
group_by(Depression) %>%
mutate(prop = count / sum(count)) %>%
ggplot(aes(x = Depression, y = prop, fill = {{fill_var}})) +
geom_bar(stat = "identity", position = "stack") +
theme_minimal() +
labs(title = title, x = xlab, y = ylab, fill = fill_lab)
}
df %>%
mutate(Depression = factor(case_when(Depression <= 3 ~ "Low", Depression > 3 & Depression < 7 ~ "Medium", Depression >= 7 ~ "High"),
levels = c("Low", "Medium", "High"), ordered = TRUE)) %>%
ggplot(aes(x = Depression, y = Age, fill = Depression)) +
geom_boxplot() +
theme_minimal() +
theme(legend.position = "none") +
labs(title = "Związek między wiekiem słuchającego a poziomem depresji", x = "Poziom depresji", y = "Wiek słuchacza")
