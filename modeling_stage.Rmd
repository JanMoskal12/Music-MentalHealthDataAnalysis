---
title: "Music & Mental Health - Modeling Stage"
author: "Jan Moskal i Szymon Makulec"
date: "2025-05-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE, message=FALSE, warning=FALSE}
library(tidymodels)
library(doParallel)
library(discrim)
library(mda)
library(themis)
```


### Wczytanie danych
```{r}
df <- readRDS("data_no_missing_values.rds")
```

### Podział danych na zbiór treningowy i testowy
```{r}
set.seed(123)

df_split <- initial_split(df, prop = 0.75, strata = "Depression")
df_train <- training(df_split)
df_test <- testing(df_split)
```

Nie mamy zbyt wiele danych do testowania, dlatego do tej części uczenia użyjemy 75% danych.

### Inicjalizacja modeli do uczenia
```{r}
models <- list(
            lda = discrim_linear(
              regularization_method = "diagonal",
              penalty = tune(),
              mode = "classification") %>% 
              set_engine("mda"),
              
            rf = rand_forest(
              trees = tune(),
              min_n = tune(),
              mtry = tune(),
              mode = "classification") %>%
              set_engine("ranger"),
  
            xgb = boost_tree(
              trees = tune(),
              min_n = tune(),
              mtry = tune(),
              learn_rate = tune(),
              mode = "classification") %>%
              set_engine("xgboost"),
  
            svm = svm_rbf(
              cost = tune(),
              rbf_sigma = tune(),
              mode = "classification") %>%
              set_engine("kernlab")
)
```

Używamy tych modeli które wytypowaliśmy w fazie wstępnego uczenia.

### Utworzenie przepisów przetwarzania danych
```{r}
recipes <- list(
            rec_lda = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.85, prefix = "pca_"),
            
            rec_rf = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors())  %>%
                        step_dummy(all_nominal_predictors()) %>% 
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.85, prefix = "pca_") %>% 
                        step_nzv(all_predictors()),

            rec_xgb = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.85, prefix = "pca_")%>% 
                        step_nzv(all_predictors()),
            
            rec_svm = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.8, prefix = "pca_") 
)
```

Dobór preprocessingów został dokonany ze względu na to jaka jest struktura naszego zbioru danych, tj. mamy wiele kolumn typu factor oraz powiązanych ze sobą ze względu na dziedzinę jak chociażby częstotliwość słuchania różnych gatunków. Ponadto odpowiedzi są subiektywne dlatego nie niosą czystej informacji.

### Utworzenie przepływów modelowania

```{r}
class_models <- workflow_set(
                  preproc = recipes,
                  models = models,
                  cross = FALSE
)
```

Tym razem uczenie będzie przebiegało w formie jeden model do jednego przepisu.

### Generowanie foldów walidacyjnych
```{r}
cv_folds <- vfold_cv(df_train, v = 5, strata = "Depression")
```

Wykrozystamy 5 krotną walidację krzyżową.

### Trenowanie modeli z walidacją krzyżową i tuningiem siatki hiperparametrów
```{r}
registerDoParallel(cores = parallel::detectCores() - 1)
controlGrid <- control_grid(save_pred = TRUE, save_workflow = TRUE, verbose = TRUE)

class_models <- workflow_map(
                  class_models,
                  "tune_grid", 
                  resamples = cv_folds,
                  grid = 50,
                  control = controlGrid,
                  metrics = metric_set(mn_log_loss, kap),
                  verbose = TRUE,
)
```

model będzie optymalizowany pod względem metryki mn_log_loss, która jest odpowiednia dla problemów wieloklasowych. Używamy 50 punktów w siatce hiperparametrów, co powinno dać nam wystarczającą różnorodność do znalezienia dobrych parametrów.

### Zapisywanie wyników 
```{r}
#saveRDS(class_models, "mxmh_models_kap3.rds")
```

### Ocena modeli
```{r}
class_models <- readRDS("mxmh_models_kap3.rds")
metrics <- collect_metrics(class_models)
metrics %>% 
  arrange(.metric, desc(mean))
```

### Najlepsze parametry dla każdego z 4 modeli

```{r}
best_params_list <- class_models %>%
  mutate(best = map(result, ~ select_best(.x, metric = "mn_log_loss"))) %>%
  select(wflow_id, best)


lda_params <- best_params_list$best[[1]]
rf_params <- best_params_list$best[[2]]
xgb_params <- best_params_list$best[[3]]
svm_params <- best_params_list$best[[4]]
```

Tworzymy listę najlepszych parametrów dla każdego z modeli startowych.


### Deklaracja modeli z najlepszymi parametrami

```{r}
models_final <- list(
            lda = discrim_linear(
              regularization_method = "diagonal",
              penalty = lda_params[[1]],
              mode = "classification") %>% 
              set_engine("mda"),
              
            rf = rand_forest(
              trees = rf_params[[2]],
              min_n = rf_params[[3]],
              mtry = rf_params[[1]],
              mode = "classification") %>%
              set_engine("ranger"),
  
            xgb = boost_tree(
              trees = xgb_params[[2]],
              min_n = xgb_params[[3]],
              mtry = xgb_params[[1]]	,
              learn_rate = xgb_params[[4]],
              mode = "classification") %>%
              set_engine("xgboost"),
  
            svm = svm_rbf(
              cost = svm_params[[1]],
              rbf_sigma = svm_params[[2]],
              mode = "classification") %>%
              set_engine("kernlab")
)
```

Aktualizujemy modele przypisując im konkretne hiper-parametry, które zostały wybrane w poprzednim kroku.

```{r}
recipes <- list(
            rec_lda = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.9, prefix = "pca_"),
            
            rec_rf = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors())  %>%
                        step_dummy(all_nominal_predictors()) %>% 
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.9, prefix = "pca_") %>% 
                        step_nzv(all_predictors()),

            rec_xgb = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.9, prefix = "pca_")%>% 
                        step_nzv(all_predictors()),
            
            rec_svm = recipe(Depression ~ ., data = df_train) %>%
                        step_normalize(all_numeric_predictors()) %>%
                        step_dummy(all_nominal_predictors()) %>%
                        step_pca(starts_with("Frequency"), num_comp = 12) %>% 
                        step_pca(all_predictors(), threshold = 0.9, prefix = "pca_") 
)
```

Tym razem nie przeprowadzamy walidacji krzyżowej więc pozwalamy sobie na to by modele nauczyły się na większej ilości danych.

### Deklaracja workflowów z ostatecznymi modelami i przepisami

```{r}
# LDA
final_wf_lda <- workflow() %>%
  add_model(models_final$lda) %>%
  add_recipe(recipes$rec_lda)

final_fit_lda <- last_fit(
  final_wf_lda,
  split = df_split,
  metrics = metric_set(mn_log_loss)
)

# Random Forest
final_wf_rf <- workflow() %>%
  add_model(models_final$rf) %>%
  add_recipe(recipes$rec_rf)

final_fit_rf <- last_fit(
  final_wf_rf,
  split = df_split,
  metrics = metric_set(mn_log_loss)
)

# XGBoost
final_wf_xgb <- workflow() %>%
  add_model(models_final$xgb) %>%
  add_recipe(recipes$rec_xgb)

final_fit_xgb <- last_fit(
  final_wf_xgb,
  split = df_split,
  metrics = metric_set(mn_log_loss)
)

# SVM
final_wf_svm <- workflow() %>%
  add_model(models_final$svm) %>%
  add_recipe(recipes$rec_svm)

final_fit_svm <- last_fit(
  final_wf_svm,
  split = df_split,
  metrics = metric_set(mn_log_loss)
)
```

Przeprowadzamy ostateczne uczenie modeli, tym razem z konkretnymi hiperparametrami i na całym zbiorze treningowym.

### Nauka oraz ocena ostatecznych modeli

```{r}
levels_order <- c("Low", "Medium", "High")

adjust_levels <- function(pred_df) {
  pred_df$.pred_class <- factor(pred_df$.pred_class, levels = levels_order, ordered = TRUE)
  pred_df$Depression <- factor(pred_df$Depression, levels = levels_order, ordered = TRUE)
  pred_df
}

preds_lda <- bind_cols(
  predict(fit(final_wf_lda, df_train), df_test, type = "prob"),
  predict(fit(final_wf_lda, df_train), df_test) %>% select(.pred_class),
  df_test %>% select(Depression)
) %>% adjust_levels()

preds_rf <- bind_cols(
  predict(fit(final_wf_rf, df_train), df_test, type = "prob"),
  predict(fit(final_wf_rf, df_train), df_test) %>% select(.pred_class),
  df_test %>% select(Depression)
) %>% adjust_levels()

preds_xgb <- bind_cols(
  predict(fit(final_wf_xgb, df_train), df_test, type = "prob"),
  predict(fit(final_wf_xgb, df_train), df_test) %>% select(.pred_class),
  df_test %>% select(Depression)
) %>% adjust_levels()

preds_svm <- bind_cols(
  predict(fit(final_wf_svm, df_train), df_test, type = "prob"),
  predict(fit(final_wf_svm, df_train), df_test) %>% select(.pred_class),
  df_test %>% select(Depression)
) %>% adjust_levels()

conf_df <- tibble(
  model = c("lda", "rf", "xgb", "svm"),
  confusion_matrix = list(
    conf_mat(preds_lda, truth = Depression, estimate = .pred_class),
    conf_mat(preds_rf,  truth = Depression, estimate = .pred_class),
    conf_mat(preds_xgb, truth = Depression, estimate = .pred_class),
    conf_mat(preds_svm, truth = Depression, estimate = .pred_class)
  )
)
```

### Macierze kontyngencji dla ostatecznych modeli

```{r}
conf_df$confusion_matrix
```
Macierze kontygnencji dla kolejno modeli lda, rf, xgb oraz svm. Widać, że modele nie poradziły sobie najlepiej z zadaniem. Największe problemy występują z przewidywaniem klasy medium, może to sugerować, że niedoceniliśmy losowości w samoocenie depresji wśród respondentów i lepszym podziałem mógł okazać się podział na dwie klasy, bądź dalej 3 klasy, ale w innych kwantylach. Pocieszający jest  fakt, że modele nie radzą sobie najgorzej z przewidywaniem klasy low i high osiągając przeciętnie 50% dokładności w tych klasach co jest lepsze niż model losowy, który dałby 33%.


### Metryki

```{r}
metric_eval <- metric_set(mn_log_loss, kap)

preds_train <- list(
  lda = bind_cols(
    predict(fit(final_wf_lda, df_train), df_train, type = "prob"),
    predict(fit(final_wf_lda, df_train), df_train) %>% select(.pred_class),
    df_train %>% select(Depression)
  ) %>% adjust_levels(),
  
  rf = bind_cols(
    predict(fit(final_wf_rf, df_train), df_train, type = "prob"),
    predict(fit(final_wf_rf, df_train), df_train) %>% select(.pred_class),
    df_train %>% select(Depression)
  ) %>% adjust_levels(),
  
  xgb = bind_cols(
    predict(fit(final_wf_xgb, df_train), df_train, type = "prob"),
    predict(fit(final_wf_xgb, df_train), df_train) %>% select(.pred_class),
    df_train %>% select(Depression)
  ) %>% adjust_levels(),
  
  svm = bind_cols(
    predict(fit(final_wf_svm, df_train), df_train, type = "prob"),
    predict(fit(final_wf_svm, df_train), df_train) %>% select(.pred_class),
    df_train %>% select(Depression)
  ) %>% adjust_levels()
)

preds_test <- list(
  lda = preds_lda,
  rf  = preds_rf,
  xgb = preds_xgb,
  svm = preds_svm
)

get_metrics <- function(pred_df) {
  metric_set(mn_log_loss, kap)(
    pred_df,
    truth = Depression,
    estimate = .pred_class,
    .pred_Low, .pred_Medium, .pred_High
  )
}

train_metrics <- map_dfr(names(preds_train), ~ {
  get_metrics(preds_train[[.x]]) %>%
    mutate(model = .x, dataset = "df_train")
})

test_metrics <- map_dfr(names(preds_test), ~ {
  get_metrics(preds_test[[.x]]) %>%
    mutate(model = .x, dataset = "df_test")
})

final_metrics <- bind_rows(train_metrics, test_metrics) %>%
  select(model, dataset, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

final_metrics %>% 
  arrange(dataset, mn_log_loss, desc(kap))

```

Ogromne przeuczenie modelu random forest, duże przeuczenie dla xgboost, svm. Model lda jako jedyny wyszedł stabilny dlatego to go wybieramy jako model ostateczny.
